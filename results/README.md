# results

This folder stores **auto-generated experimental outputs**:

- Publication-ready figures (PNG)
- Summary tables (CSV), e.g. `real_calls_clean.csv`, `summary_metrics.csv`

These files are produced by the main script:

```bash
python src/experiment_real_plus.py --data DATA/erm2-nwe9.csv


Key outputs include:

hist_delay.png, kde_delay.png – response-time distribution

category_pie.png, box_delay_by_category.png – category distributions

heatmap_weekday_hour.png – temporal patterns

metrics_bar_ci.png, pr_curves_all.png, pr_curves_with_ae.png – model comparison

rf_feature_importance.png, shap_* – interpretability

summary_metrics.csv – consolidated metrics table

real_calls_clean.csv – cleaned and labeled dataset used by the models

Reproducibility:
All figures and tables in the paper can be regenerated by rerunning
src/experiment_real_plus.py. The results000/ folder keeps an earlier
experimental run and is not required for the main reproduction.



---

## 4. `docs/data_sources.md`

在 `docs` 目录下新建一个更详细的说明（名字你也可以叫别的，比如 `DATA_SOURCES.md`，但 README 中已经提到 `data_sources.md` 就按这个）：

```md
# Data Sources & Preprocessing

This project uses **public NYC 311 service-request data** as a proxy for
nurse-call–like service tickets.

## 1. Raw Data

- Dataset: **311 Service Requests from 2010 to Present**
- Portal: NYC OpenData
- URL: https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9

The official data dictionary is provided in:

- `DATA/311_ServiceRequest_2010-Present_DataDictionary_Updated_2023.xlsx`

## 2. Download Instructions

1. Visit the NYC OpenData page above.
2. Click **Export** → **Download as CSV**.
3. Optionally filter by:
   - Date range (e.g., a few months) to keep the file size manageable.
   - Specific agencies (NYPD, HPD, DSNY, etc.).
4. Save the CSV to a local path and name it, for example,
   `DATA/erm2-nwe9_raw.csv`.

> The repository does **not** include the full raw CSV in order to keep
> the GitHub project lightweight and avoid version drift with NYC OpenData.

## 3. Preprocessing Overview

The script `src/experiment_real_plus.py` expects a **cleaned** CSV
(`DATA/erm2-nwe9.csv`) with derived fields.

The preprocessing steps are:

1. **Parse datetime columns**  
   Convert the creation and closing timestamps to pandas `datetime`:

   - `Created Date`
   - `Closed Date` or equivalent resolution timestamp

2. **Compute response time**  
   Create a new column:

   ```python
   df["resp_h"] = (df["Closed Date"] - df["Created Date"]).dt.total_seconds() / 3600.0


3. Filter obvious errors

Remove rows with negative or extremely large resp_h (e.g. > 200 hours).

Drop rows with missing key timestamps.

4. Create temporal features
df["hour"] = df["Created Date"].dt.hour
df["weekday"] = df["Created Date"].dt.weekday  # 0=Mon … 6=Sun

5.Create category feature
Map relevant agency fields to a higher-level category column, for
example:
df["category"] = df["Agency"]  # or a cleaned mapping of agency codes

6.Define anomaly labels
For supervised baselines (RandomForest), we construct an
is_anomaly label. One reasonable policy is:

Compute a high percentile of response time per category, e.g. 95th.

Label tickets above that percentile as is_anomaly = 1,
others as 0.

The exact labeling rule used in the paper is encoded inside
src/experiment_real_plus.py.

7. Save cleaned CSV
df.to_csv("DATA/erm2-nwe9.csv", index=False)
